# TfL Status Service Configuration

tfl {
  # Unique node identifier (override per instance)
  node-id = "node-1"
  node-id = ${?TFL_NODE_ID}

  http {
    port = 8080
    port = ${?TFL_HTTP_PORT}
    ask-timeout = 5s
    response-timeout = 10s
  }

  refresh {
    # === POLLING ===
    # How often each node checks for fresh data
    interval = 30s
    jitter = 5s

    # === CRDT PEER CHECK (on RefreshTick) ===
    # If peer data is fresher than this, skip TfL call and use peer data
    # Set higher than background-refresh to avoid redundant TfL calls across cluster
    recent-enough-threshold = 15s

    # === PROACTIVE REFRESH (on client request) ===
    # If cache satisfies client's maxAgeMs but is older than this,
    # trigger background refresh to keep data warm for next request
    # Should be < recent-enough-threshold to avoid wasted refreshes
    background-refresh-threshold = 10s

    scatter-gather-timeout = 2s

    # === CLIENT REQUEST BOUNDS ===
    # Freshness floor: minimum freshness clients can request
    # Requests for tighter freshness are silently upgraded to this value
    # Rationale: tube status doesn't change faster than every few seconds,
    # and allowing sub-second freshness lets one client exhaust TfL quota
    # or trigger circuit breaker for everyone
    minimum-freshness-ms = 5000  # 5 seconds - tube status domain floor

    # Default freshness when client doesn't specify maxAgeMs
    # Generous default encourages cache hits, clients can override if needed
    default-freshness-ms = 60000  # 60 seconds
  }

  rate-limit {
    requests-per-minute = 100
  }

  circuit-breaker {
    failure-threshold = 5
    open-duration = 30s
  }

  retry {
    max-retries = 3
    base-delay = 1s
    max-delay = 30s
    jitter-factor = 0.25
  }
}

# Pekko configuration
pekko {
  loglevel = "INFO"
  stdout-loglevel = "INFO"

  actor {
    provider = "cluster"

    serialization-bindings {
      "com.ig.tfl.model.TubeStatus" = jackson-json
    }
  }

  remote.artery {
    canonical {
      hostname = "127.0.0.1"
      hostname = ${?PEKKO_HOST}
      port = 2551
      port = ${?PEKKO_PORT}
    }
  }

  cluster {
    seed-nodes = [
      "pekko://tfl-cluster@127.0.0.1:2551"
    ]
    seed-nodes = ${?PEKKO_SEED_NODES}

    downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"

    # Multi-datacenter support (production)
    # Tag nodes with their DC for locality-aware replication
    multi-data-center {
      self-data-center = "dc-local"
      self-data-center = ${?PEKKO_DC}
      # Cross-DC failure detection is more lenient (WAN latency)
      failure-detector {
        acceptable-heartbeat-pause = 10s
      }
    }

    # Distributed Data configuration - aggressive replication
    distributed-data {
      # Fast gossip for quick convergence (default is 2s)
      gossip-interval = 200ms
      # Fast subscriber notification
      notify-subscribers-interval = 100ms
      # Use delta-CRDT for efficient replication
      delta-crdt.enabled = on

      # Cross-DC replication (when multi-DC enabled)
      # Prefer local DC, async replicate to remote
      # prefer-oldest = on  # Uncomment for DC-aware read/write
    }
  }

  # HTTP settings
  http {
    server {
      idle-timeout = 60s
      request-timeout = 30s
    }
  }

  # Serialization for CRDT
  serialization.jackson {
    jackson-json {
      serialization-features {
        WRITE_DATES_AS_TIMESTAMPS = off
      }
    }
  }
}

# Logback configuration reference
# See logback.xml for detailed logging configuration
